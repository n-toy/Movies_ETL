{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Movies ETL Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "from config import db_password\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs:\n",
    "    1. wikipedia.movies.json\n",
    "        scraped JSON of wikipedia movie data\n",
    "    2. movies_metadata.csv\n",
    "        Kaggle dataset of movies, and movie dataset from The Movie Database (TMDb)\n",
    "    3. ratings.csv\n",
    "        Dataset of all TMDb movie reviews (over 26 million reviews)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nathan\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#File directory location that contains inputs:\n",
    "file_dir = r\"C:\\Users\\Nathan\\Documents\\Data Bootcamp\\Movies_ETL\\Resources\"\n",
    "\n",
    "#Read in inputs:\n",
    "with open(f'{file_dir}/wikipedia.movies.json', mode='r') as file:\n",
    "    wiki_movies_raw = json.load(file)\n",
    "    \n",
    "kaggle_metadata = pd.read_csv(f'{file_dir}/movies_metadata.csv')\n",
    "ratings = pd.read_csv(f'{file_dir}/ratings_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def movies_etl(wiki_movies_raw,kaggle_metadata, ratings):\n",
    "    \n",
    "#Read Inputs:\n",
    "\n",
    "    #Create a Pandas DF from raw JSON\n",
    "    wiki_movies_df = pd.DataFrame(wiki_movies_raw)\n",
    "    \n",
    "    #Grab movies where there is a director/directed by and is a movie\n",
    "    wiki_movies = [movie for movie in wiki_movies_raw\n",
    "                   if ('Director' in movie or 'Directed by' in movie)\n",
    "                   and 'imdb_link' in movie\n",
    "                   and 'No. of episodes' not in movie]\n",
    "    \n",
    "    #clean_movie function that searches for alternate title name of the movie in language columns.\n",
    "    #If Alt name found append to a list for title name, and then pop \n",
    "    def clean_movie(movie):\n",
    "        movie = dict(movie) #create a non-destructive copy\n",
    "        alt_titles = {}\n",
    "        for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                    'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                    'Mandarin','McCune–Reischauer','Original title','Polish',\n",
    "                    'Revised Romanization','Romanized','Russian',\n",
    "                    'Simplified','Traditional','Yiddish']:\n",
    "            if key in movie:\n",
    "                alt_titles[key] = movie[key]\n",
    "                movie.pop(key)\n",
    "        if len(alt_titles) > 0:\n",
    "            movie['alt_titles'] = alt_titles\n",
    "    \n",
    "        #Function to alter column names. We have a lot of duplicated columns, so lets try to combine them using pop\n",
    "        def change_column_name(old_name,new_name):\n",
    "            if old_name in movie:\n",
    "                movie[new_name] = movie.pop(old_name)\n",
    "\n",
    "        change_column_name('Adaptation by', 'Writer(s)')\n",
    "        change_column_name('Country of origin', 'Country')\n",
    "        change_column_name('Directed by', 'Director')\n",
    "        change_column_name('Distributed by', 'Distributor')\n",
    "        change_column_name('Edited by', 'Editor(s)')\n",
    "        change_column_name('Length', 'Running time')\n",
    "        change_column_name('Original release', 'Release date')\n",
    "        change_column_name('Music by', 'Composer(s)')\n",
    "        change_column_name('Produced by', 'Producer(s)')\n",
    "        change_column_name('Producer', 'Producer(s)')\n",
    "        change_column_name('Productioncompanies ', 'Production company(s)')\n",
    "        change_column_name('Productioncompany ', 'Production company(s)')\n",
    "        change_column_name('Released', 'Release Date')\n",
    "        change_column_name('Release Date', 'Release date')\n",
    "        change_column_name('Screen story by', 'Writer(s)')\n",
    "        change_column_name('Screenplay by', 'Writer(s)')\n",
    "        change_column_name('Story by', 'Writer(s)')\n",
    "        change_column_name('Theme music composer', 'Composer(s)')\n",
    "        change_column_name('Written by', 'Writer(s)')\n",
    "    \n",
    "        return movie\n",
    "    \n",
    "    #Clean movies becomes our wiki_movies_df\n",
    "    wiki_movies_df = pd.DataFrame([clean_movie(movie) for movie in wiki_movies])\n",
    "    \n",
    "    #Populate imdb_ID IF data issue with imdb_link contains the id field. Use RegEx\n",
    "    wiki_movies_df['imdb_id'] = wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "    \n",
    "    #Get imdb_id as UNIQUE. Drop duplicates\n",
    "    wiki_movies_df.drop_duplicates(subset='imdb_id', inplace=True)\n",
    "    \n",
    "    #Only want columns that are less than 90% null values.\n",
    "    wiki_columns_to_keep = [column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9]\n",
    "    wiki_movies_df = wiki_movies_df[wiki_columns_to_keep]\n",
    "    \n",
    "    #Handle and sanitize Box Office Input\n",
    "    #Grab all entries where Box Office is not NaN\n",
    "    box_office = wiki_movies_df['Box office'].dropna()\n",
    "    #RegEx Expreissions for Box Office value inputs. \n",
    "    \n",
    "    form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n",
    "    form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)'\n",
    "    #Parse_dollars function that will take and sanitize input to convert to FLOAT\n",
    "    def parse_dollars(s):\n",
    "        # if s is not a string, return NaN\n",
    "        if type(s) != str:\n",
    "            return np.nan\n",
    "        # if input is of the form $###.# million\n",
    "        if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.IGNORECASE):\n",
    "            # remove dollar sign and \" million\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "            # convert to float and multiply by a million\n",
    "            value = float(s) * 10**6\n",
    "            # return value\n",
    "            return value\n",
    "        # if input is of the form $###.# billion\n",
    "        elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.IGNORECASE):\n",
    "            # remove dollar sign and \" billion\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "            # convert to float and multiply by a billion\n",
    "            value = float(s) * 10**9\n",
    "            # return value\n",
    "            return value\n",
    "        # if input is of the form $###,###,###\n",
    "        elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.IGNORECASE):\n",
    "            # remove dollar sign and commas\n",
    "            s = re.sub('\\$|,','', s)\n",
    "            # convert to float\n",
    "            value = float(s)\n",
    "            # return value\n",
    "            return value\n",
    "        # otherwise, return NaN\n",
    "        else:\n",
    "            return np.nan\n",
    "    #Apply formula to all values matching RegEx Expressions for box office    \n",
    "    wiki_movies_df['box_office'] = box_office.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    #Drop original column\n",
    "    wiki_movies_df.drop('Box office', axis=1, inplace=True)\n",
    "    \n",
    "    #Repeat sanitize steps for budget:\n",
    "    budget = wiki_movies_df['Budget'].dropna()\n",
    "    \n",
    "    # Convert any lists to strings:\n",
    "    budget = budget.map(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    #Remove any values between a dollar sign and a hyphen\n",
    "    budget = budget.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "    #Remove any square brackets in our values\n",
    "    budget = budget.str.replace(r'\\[\\d+\\]\\s*', '')\n",
    "    \n",
    "    #Apply formula to all values matching RegEx Expressions for budget    \n",
    "    wiki_movies_df['budget'] = budget.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "\n",
    "    #can drop the old 'Budget' Column since we have a new and updated one. \n",
    "    wiki_movies_df.drop('Budget', axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    #Convert release date from UNIX to datetime:\n",
    "    release_date = wiki_movies_df['Release date'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    #Date forms to be parsed through:\n",
    "    date_form_one = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s[123]\\d,\\s\\d{4}'\n",
    "    date_form_two = r'\\d{4}.[01]\\d.[123]\\d'\n",
    "    date_form_three = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}'\n",
    "    date_form_four = r'\\d{4}'\n",
    "    #Pandas to_datetime \n",
    "    wiki_movies_df['release_date'] = pd.to_datetime(release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})')[0], infer_datetime_format=True)\n",
    "\n",
    "    # Running Time\n",
    "    running_time = wiki_movies_df['Running time'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    # RegEx expression to sanitize all values. Extract out full minutes, and hours from string\n",
    "    running_time_extract = running_time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m')\n",
    "    #Apply formula to all values matching RegEx Expressions for running time\n",
    "    wiki_movies_df['running_time'] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)\n",
    "    # Drop the old column\n",
    "    wiki_movies_df.drop('Running time', axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "#KAGGLE MOVIES METADATA\n",
    "    # Keep only non-adult movies from Kaggle Dataset\n",
    "    kaggle_metadata = kaggle_metadata[kaggle_metadata['adult'] == 'False'].drop('adult',axis='columns')\n",
    "    #Convert the rest of the mismatched column types to numeric:\n",
    "    kaggle_metadata['budget'] = kaggle_metadata['budget'].astype(int)\n",
    "    kaggle_metadata['id'] = pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "    kaggle_metadata['popularity'] = pd.to_numeric(kaggle_metadata['popularity'], errors='raise')\n",
    "\n",
    "    #Convert this to to_datetime using pandas:\n",
    "    kaggle_metadata['release_date'] = pd.to_datetime(kaggle_metadata['release_date'])\n",
    "\n",
    "    \n",
    "#RATINGS\n",
    "    #Convert unix timestamp to datetime\n",
    "    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')\n",
    "    \n",
    "    \n",
    "#MERGING DATA TOGETHER\n",
    "    movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "    \n",
    "    #Drop this one movie because the release date is off:\n",
    "    try:\n",
    "        movies_df = movies_df.drop(movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')].index)\n",
    "    except: \n",
    "        pass\n",
    "    #Drop these columns from the dataset. We'll use the Kaggle version of these columns/don't need these columns\n",
    "    movies_df.drop(columns=['title_wiki','release_date_wiki','Language','Production company(s)'], inplace=True)\n",
    "    \n",
    "    #function that fills in missing data for a column pair, and then drops the redundant column\n",
    "    def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "        df[kaggle_column] = df.apply(\n",
    "            lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column]\n",
    "            , axis=1)\n",
    "        df.drop(columns=wiki_column, inplace=True)\n",
    "    \n",
    "    fill_missing_kaggle_data(movies_df, 'runtime', 'running_time')\n",
    "    fill_missing_kaggle_data(movies_df, 'budget_kaggle', 'budget_wiki')\n",
    "    fill_missing_kaggle_data(movies_df, 'revenue', 'box_office')\n",
    "\n",
    "    #Reordering the df to make more sense\n",
    "    movies_df = movies_df.loc[:, ['imdb_id','id','title_kaggle','original_title','tagline','belongs_to_collection','url','imdb_link',\n",
    "                           'runtime','budget_kaggle','revenue','release_date_kaggle','popularity','vote_average','vote_count',\n",
    "                           'genres','original_language','overview','spoken_languages','Country',\n",
    "                           'production_companies','production_countries','Distributor',\n",
    "                           'Producer(s)','Director','Starring','Cinematography','Editor(s)','Writer(s)','Composer(s)','Based on'\n",
    "                          ]]\n",
    "\n",
    "    #rename the columns to be consistent:\n",
    "    movies_df.rename({'id':'kaggle_id',\n",
    "                      'title_kaggle':'title',\n",
    "                      'url':'wikipedia_url',\n",
    "                      'budget_kaggle':'budget',\n",
    "                      'release_date_kaggle':'release_date',\n",
    "                      'Country':'country',\n",
    "                      'Distributor':'distributor',\n",
    "                      'Producer(s)':'producers',\n",
    "                      'Director':'director',\n",
    "                      'Starring':'starring',\n",
    "                      'Cinematography':'cinematography',\n",
    "                      'Editor(s)':'editors',\n",
    "                      'Writer(s)':'writers',\n",
    "                      'Composer(s)':'composers',\n",
    "                      'Based on':'based_on'\n",
    "                     }, axis='columns', inplace=True)    \n",
    "    \n",
    "##RATINGS APPEND\n",
    "    #Count number of reviews per Movie ID. \n",
    "    #Rename the column UserID to Count. Doesn't matter that we rename userID it has arbitrary data in it for a count\n",
    "    #Pivot the data so that the index is MovieID, columns will be rating values, and rows will be the counts for each rating value.\n",
    "    rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                    .rename({'userId':'count'}, axis=1) \\\n",
    "                    .pivot(index='movieId',columns='rating', values='count')\n",
    "\n",
    "    #Prepend the rating_ to each column\n",
    "    rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "\n",
    "    #Merge ratings data\n",
    "    movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')\n",
    "\n",
    "    # Fill in 0's for missing values and we good:\n",
    "    movies_with_ratings_df[rating_counts.columns] = movies_with_ratings_df[rating_counts.columns].fillna(0)\n",
    "\n",
    "    return movies_df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = movies_etl(wiki_movies_raw,kaggle_metadata, ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies_df table data TRUNCATED.\n",
      "movies_df Done. 1.6535015106201172 total seconds elapsed\n",
      "importing rows 0 to 1000000...Done. 138.03849864006042 total seconds elapsed\n",
      "importing rows 1000000 to 2000000...Done. 231.5364739894867 total seconds elapsed\n",
      "importing rows 2000000 to 3000000...Done. 322.5301375389099 total seconds elapsed\n",
      "importing rows 3000000 to 4000000...Done. 414.3806154727936 total seconds elapsed\n",
      "importing rows 4000000 to 5000000...Done. 504.58860087394714 total seconds elapsed\n",
      "importing rows 5000000 to 6000000...Done. 597.2856662273407 total seconds elapsed\n",
      "importing rows 6000000 to 7000000...Done. 687.3074865341187 total seconds elapsed\n",
      "importing rows 7000000 to 8000000...Done. 780.1738848686218 total seconds elapsed\n",
      "importing rows 8000000 to 9000000...Done. 873.4825332164764 total seconds elapsed\n",
      "importing rows 9000000 to 10000000...Done. 966.6141765117645 total seconds elapsed\n",
      "importing rows 10000000 to 11000000...Done. 1059.39750289917 total seconds elapsed\n",
      "importing rows 11000000 to 12000000...Done. 1152.3243081569672 total seconds elapsed\n",
      "importing rows 12000000 to 13000000...Done. 1241.8857069015503 total seconds elapsed\n",
      "importing rows 13000000 to 14000000...Done. 1334.4256029129028 total seconds elapsed\n",
      "importing rows 14000000 to 15000000...Done. 1426.600548505783 total seconds elapsed\n",
      "importing rows 15000000 to 16000000...Done. 1521.6989431381226 total seconds elapsed\n",
      "importing rows 16000000 to 17000000...Done. 1613.3289563655853 total seconds elapsed\n",
      "importing rows 17000000 to 18000000...Done. 1705.1711988449097 total seconds elapsed\n",
      "importing rows 18000000 to 19000000...Done. 1797.5652532577515 total seconds elapsed\n",
      "importing rows 19000000 to 20000000...Done. 1891.0790565013885 total seconds elapsed\n",
      "importing rows 20000000 to 21000000...Done. 1981.2237980365753 total seconds elapsed\n",
      "importing rows 21000000 to 22000000...Done. 2074.1152226924896 total seconds elapsed\n",
      "importing rows 22000000 to 23000000...Done. 2168.4069221019745 total seconds elapsed\n",
      "importing rows 23000000 to 24000000...Done. 2258.5148179531097 total seconds elapsed\n",
      "importing rows 24000000 to 25000000...Done. 2350.638545513153 total seconds elapsed\n",
      "importing rows 25000000 to 26000000...Done. 2441.2188692092896 total seconds elapsed\n",
      "importing rows 26000000 to 26024289...Done. 2443.0695514678955 total seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db_string = f\"postgres://postgres:{db_password}@127.0.0.1:5432/movie_data\"\n",
    "engine = create_engine(db_string)\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    engine.execute('DELETE FROM movies;')\n",
    "    engine.execute('commit;')\n",
    "    print('movies_df table data TRUNCATED.')\n",
    "except:\n",
    "    pass\n",
    "movies_df.to_sql(name = 'movies',con = engine,if_exists = 'append')\n",
    "print(f'movies_df Done. {time.time() - start_time} total seconds elapsed')\n",
    "\n",
    "\n",
    "#Ratings Data full import\n",
    "rows_imported = 0\n",
    "start_time = time.time()\n",
    "\n",
    "try: \n",
    "    engine.execute('DELETE FROM ratings;')\n",
    "    engine.execute('commit;')\n",
    "except: \n",
    "    pass\n",
    "for data in pd.read_csv(f'{file_dir}/ratings.csv', chunksize=1000000):\n",
    "    # print out the range of rows that are being imported\n",
    "    print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    data.to_sql(name='ratings', con=engine, if_exists='append',method = 'multi')\n",
    "    # increment the number of rows imported by the size of 'data'\n",
    "    rows_imported += len(data)\n",
    "    # print that the rows have finished importing\n",
    "    print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
